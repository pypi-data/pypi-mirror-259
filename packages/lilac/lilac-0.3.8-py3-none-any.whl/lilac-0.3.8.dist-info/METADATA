Metadata-Version: 2.1
Name: lilac
Version: 0.3.8
Summary: Organize unstructured data
Home-page: https://github.com/lilacai/lilac
License: Apache-2.0
Author: Lilac AI Inc.
Author-email: info@lilacml.com
Requires-Python: >=3.9,<4.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Provides-Extra: all
Provides-Extra: bge
Provides-Extra: cohere
Provides-Extra: embeddings
Provides-Extra: github
Provides-Extra: gmail
Provides-Extra: gte
Provides-Extra: lang-detection
Provides-Extra: langsmith
Provides-Extra: llms
Provides-Extra: nomic
Provides-Extra: openai
Provides-Extra: pii
Provides-Extra: sbert
Provides-Extra: signals
Provides-Extra: sources
Provides-Extra: text-stats
Requires-Dist: FlagEmbedding (>=1.2.3,<2.0.0) ; extra == "all" or extra == "bge"
Requires-Dist: authlib (>=1.2.1,<2.0.0)
Requires-Dist: click (>=8.1.3,<9.0.0)
Requires-Dist: cloudpickle (>=2.0.0,<3.0.0)
Requires-Dist: cohere (>=4.32,<5.0) ; extra == "all" or extra == "embeddings" or extra == "cohere"
Requires-Dist: datasets (>=2.12.0,<3.0.0)
Requires-Dist: detect-secrets (>=1.4.0,<2.0.0) ; extra == "all" or extra == "signals" or extra == "pii"
Requires-Dist: duckdb (>=0.9.2,<0.10.0)
Requires-Dist: einops (>=0.7.0,<0.8.0) ; extra == "nomic"
Requires-Dist: email-reply-parser (>=0.5.12,<0.6.0) ; extra == "all" or extra == "gmail"
Requires-Dist: fastapi (>=0.109.1,<0.110.0)
Requires-Dist: fsspec (>=2023.9.2,<2024.0.0)
Requires-Dist: gcsfs (>=2023.9.2,<2024.0.0)
Requires-Dist: google-api-python-client (>=2.88.0,<3.0.0) ; extra == "all" or extra == "gmail"
Requires-Dist: google-auth-httplib2 (>=0.1.0,<0.2.0) ; extra == "all" or extra == "gmail"
Requires-Dist: google-auth-oauthlib (>=1.0.0,<2.0.0) ; extra == "all" or extra == "gmail"
Requires-Dist: google-cloud-storage (>=2.5.0,<3.0.0)
Requires-Dist: gunicorn (>=21.2.0,<22.0.0)
Requires-Dist: hdbscan (>=0.8.33,<0.9.0) ; extra == "all" or extra == "signals"
Requires-Dist: hnswlib (>=0.8.0,<0.9.0)
Requires-Dist: httpx (>=0.24.1,<0.25.0)
Requires-Dist: instructor (>=0.4.0,<0.5.0)
Requires-Dist: itsdangerous (>=2.1.2,<3.0.0)
Requires-Dist: jinja2 (>=3.1.3,<4.0.0)
Requires-Dist: joblib (>=1.3.1,<2.0.0)
Requires-Dist: langdetect (>=1.0.9,<2.0.0) ; extra == "all" or extra == "signals" or extra == "lang-detection"
Requires-Dist: langsmith (>=0.0.41,<0.0.42) ; extra == "all" or extra == "sources" or extra == "langsmith"
Requires-Dist: llama-hub (>=0.0.67,<0.0.68) ; (python_version >= "3.9" and python_version < "3.12") and (extra == "all" or extra == "github")
Requires-Dist: llama-index (>=0.10,<0.11) ; (python_version >= "3.9" and python_version < "3.12") and (extra == "all" or extra == "github")
Requires-Dist: loky (>=3.4.1,<4.0.0)
Requires-Dist: modal (>=0.56.4396,<0.57.0)
Requires-Dist: openai (>=1.7.1,<2.0.0) ; extra == "all" or extra == "embeddings" or extra == "llms" or extra == "openai"
Requires-Dist: orjson (>=3.8.10,<4.0.0)
Requires-Dist: pandas (>=2.0,<3.0)
Requires-Dist: pillow (>=10.2.0,<11.0.0)
Requires-Dist: presidio_analyzer (>=2.2,<3.0) ; extra == "all" or extra == "pii"
Requires-Dist: psutil (>=5.9.5,<6.0.0)
Requires-Dist: pyarrow (>=14.0.1,<15.0.0)
Requires-Dist: pydantic (>=2.5.2,<3.0.0)
Requires-Dist: python-dotenv (>=1.0.0,<2.0.0)
Requires-Dist: pyyaml (>=6.0.1,<7.0.0)
Requires-Dist: requests (>=2,<3)
Requires-Dist: scikit-learn (>=1.3.0,<2.0.0)
Requires-Dist: sentence-transformers (>=2.3.1,<3.0.0) ; extra == "all" or extra == "embeddings" or extra == "gte" or extra == "nomic" or extra == "sbert"
Requires-Dist: spacy (>=3.5.1,<4.0.0) ; extra == "all"
Requires-Dist: tenacity (>=8.2.2,<9.0.0)
Requires-Dist: textacy (>=0.13.0,<0.14.0) ; extra == "all" or extra == "signals" or extra == "text-stats"
Requires-Dist: tiktoken (>=0.5.1,<0.6.0)
Requires-Dist: tqdm (>=4.66.1,<5.0.0)
Requires-Dist: transformers (>=4.37.2,<5.0.0) ; extra == "all" or extra == "bge"
Requires-Dist: typing-extensions (==4.9.0)
Requires-Dist: umap-learn (>=0.5.4,<0.6.0) ; extra == "all"
Requires-Dist: uvicorn[standard] (>=0.23.2,<0.24.0)
Project-URL: Repository, https://github.com/lilacai/lilac
Description-Content-Type: text/markdown

<h1 align="center">Lilac</h1>
<h3 align="center" style="font-size: 20px; margin-bottom: 4px">Better data, better AI</h3>
<p align="center">
  <a style="padding: 4px;"  href="https://lilacai-lilac.hf.space/">
    <span style="margin-right: 4px; font-size: 12px">üîó</span> <span style="font-size: 14px">Try the Lilac web demo!</span>
  </a>
  <br/><br/>
  <a href="https://lilacml.com/">
        <img alt="Site" src="https://img.shields.io/badge/Site-lilacml.com-ed2dd0?link=https%3A%2F%2Flilacml.com"/>
    </a>
    <a href="https://discord.gg/jNzw9mC8pp">
        <img alt="Discord" src="https://img.shields.io/discord/1135996772280451153?label=Join%20Discord" />
    </a>
    <a href="https://github.com/lilacai/lilac/blob/main/LICENSE">
          <img alt="License Apache 2.0" src="https://img.shields.io/badge/License-Apache 2.0-blue.svg?style=flat&color=ed2dd0" height="20" width="auto">
    </a>
    <br/>
    <a href="https://github.com/lilacai/lilac">
      <img src="https://img.shields.io/github/stars/lilacai/lilac?style=social" />
    </a>
    <a href="https://twitter.com/lilac_ai">
      <img src="https://img.shields.io/twitter/follow/lilac_ai" alt="Follow on Twitter" />
    </a>
</p>

Lilac is a tool for exploration, curation and quality control of datasets for training, fine-tuning
and monitoring LLMs.

Lilac is used by companies like [Cohere](https://cohere.com/) and
[Databricks](https://www.databricks.com/) to visualize, quantify and improve the quality of
pre-training and fine-tuning data.

Lilac runs **on-device** using open-source LLMs with a UI and Python API.

## üÜí New

- [Lilac Garden](https://www.lilacml.com/#garden) is our hosted platform for blazing fast
  dataset-level computations. [Sign up](https://forms.gle/Gz9cpeKJccNar5Lq8) to join the pilot.
- Cluster & title millions of documents with the power of LLMs.
  [Explore and search](https://lilacai-lilac.hf.space/datasets#lilac/OpenOrca&query=%7B%7D&viewPivot=true&pivot=%7B%22outerPath%22%3A%5B%22question__cluster%22%2C%22category_title%22%5D%2C%22innerPath%22%3A%5B%22question__cluster%22%2C%22cluster_title%22%5D%7D)
  over 36,000 clusters of 4.3M documents in OpenOrca

## Why use Lilac?

- Explore your data interactively with LLM-powered search, filter, clustering and annotation.
- Curate AI data, applying best practices like removing duplicates, PII and obscure content to
  reduce dataset size and lower training cost and time.
- Inspect and collaborate with your team on a single, centralized dataset to improve data quality.
- Understand how data changes over time.

Lilac can offload expensive computations to [Lilac Garden](https://www.lilacml.com/#garden), our
hosted platform for blazing fast dataset-level computations.

<img alt="image" src="docs/_static/dataset/dataset_cluster_view.png">

> See our [3min walkthrough video](https://www.youtube.com/watch?v=RrcvVC3VYzQ)

## üî• Getting started

### üíª Install

```sh
pip install lilac[all]
```

If you prefer no local installation, you can duplicate our
[Spaces demo](https://lilacai-lilac.hf.space/) by following documentation
[here](https://docs.lilacml.com/deployment/huggingface_spaces.html).

For more detailed instructions, see our
[installation guide](https://docs.lilacml.com/getting_started/installation.html).

### üåê Start a webserver

Start a Lilac webserver with our `lilac` CLI:

```sh
lilac start ~/my_project
```

Or start the Lilac webserver from Python:

```py
import lilac as ll

ll.start_server(project_dir='~/my_project')
```

This will open start a webserver at http://localhost:5432/ where you can now load datasets and
explore them.

### Lilac Garden

Lilac Garden is our hosted platform for running dataset-level computations. We utilize powerful GPUs
to accelerate expensive signals like Clustering, Embedding, and PII.
[Sign up](https://forms.gle/Gz9cpeKJccNar5Lq8) to join the pilot.

- Cluster and title **a million** data points in **20 mins**
- Embed your dataset at **half a billion** tokens per min
- Run your own signal

### üìä Load data

Datasets can be loaded directly from HuggingFace, Parquet, CSV, JSON,
[LangSmith from LangChain](https://www.langchain.com/langsmith), SQLite,
[LLamaHub](https://llamahub.ai/), Pandas, Parquet, and more. More documentation
[here](https://docs.lilacml.com/datasets/dataset_load.html).

```python
import lilac as ll

ll.set_project_dir('~/my_project')
dataset = ll.from_huggingface('imdb')
```

If you prefer, you can load datasets directly from the UI without writing any Python:

<img width="600" alt="image" src="https://github.com/lilacai/lilac/assets/1100749/d5d385ce-f11c-47e6-9c00-ea29983e24f0">

### üîé Explore

<!-- prettier-ignore -->
> [!NOTE]
> üîó Explore [OpenOrca](https://lilacai-lilac.hf.space/datasets#lilac/OpenOrca) and
> [its clusters](https://lilacai-lilac.hf.space/datasets#lilac/OpenOrca&query=%7B%7D&viewPivot=true&pivot=%7B%22outerPath%22%3A%5B%22question__cluster%22%2C%22category_title%22%5D%2C%22innerPath%22%3A%5B%22question__cluster%22%2C%22cluster_title%22%5D%7D)
> before installing!

Once we've loaded a dataset, we can explore it from the UI and get a sense for what's in the data.
More documentation [here](https://docs.lilacml.com/datasets/dataset_explore.html).

<img alt="image" src="docs/_static/dataset/dataset_explore.png">

### ‚ú® Clustering

Cluster any text column to get automated dataset insights:

```python
dataset = ll.get_dataset('local', 'imdb')
dataset.cluster('text') # add `use_garden=True` to offload to Lilac Garden
```

<!-- prettier-ignore -->
> [!TIP]
> Clustering on device can be slow or impractical, especially on machines without a powerful GPU or
> large memory. Offloading the compute to [Lilac Garden](https://www.lilacml.com/#garden), our
hosted data processing platform, can speedup clustering by more than 100x.

<img alt="image" src="docs/_static/dataset/dataset_cluster_view.png">

### ‚ö° Annotate with Signals (PII, Text Statistics, Language Detection, Neardup, etc)

Annotating data with signals will produce another column in your data.

```python
dataset = ll.get_dataset('local', 'imdb')
dataset.compute_signal(ll.LangDetectionSignal(), 'text') # Detect language of each doc.

# [PII] Find emails, phone numbers, ip addresses, and secrets.
dataset.compute_signal(ll.PIISignal(), 'text')

# [Text Statistics] Compute readability scores, number of chars, TTR, non-ascii chars, etc.
dataset.compute_signal(ll.PIISignal(), 'text')

# [Near Duplicates] Computes clusters based on minhash LSH.
dataset.compute_signal(ll.NearDuplicateSignal(), 'text')

# Print the resulting manifest, with the new field added.
print(dataset.manifest())
```

We can also compute signals from the UI:

<img width="400" alt="image" src="docs/_static/dataset/dataset_compute_signal_modal.png">

### üîé Search

Semantic and conceptual search requires computing an embedding first:

```python
dataset.compute_embedding('gte-small', path='text')
```

#### Semantic search

In the UI, we can search by semantic similarity or by classic keyword search to find chunks of
documents similar to a query:

<img width="600" alt="image" src="https://github.com/lilacai/lilac/assets/1100749/4adb603e-8dca-43a3-a492-fd862e194a5a">

<img width="600" alt="image" src="https://github.com/lilacai/lilac/assets/1100749/fdee2127-250b-4e06-9ff9-b1023c03b72f">

We can run the same search in Python:

```python
rows = dataset.select_rows(
  columns=['text', 'label'],
  searches=[
    ll.SemanticSearch(
      path='text',
      embedding='gte-small')
  ],
  limit=1)

print(list(rows))
```

#### Conceptual search

Conceptual search is a much more controllable and powerful version of semantic search, where
"concepts" can be taught to Lilac by providing positive and negative examples of that concept.

Lilac provides a set of built-in concepts, but you can create your own for very specif

<img width="600" alt="image" src="https://github.com/lilacai/lilac/assets/1100749/9941024b-7c24-4d87-ae46-925f8da435e1">

We can create a concept in Python with a few examples, and search by it:

```python
concept_db = ll.DiskConceptDB()
db.create(namespace='local', name='spam')
# Add examples of spam and not-spam.
db.edit('local', 'spam', ll.concepts.ConceptUpdate(
  insert=[
    ll.concepts.ExampleIn(label=False, text='This is normal text.'),
    ll.concepts.ExampleIn(label=True, text='asdgasdgkasd;lkgajsdl'),
    ll.concepts.ExampleIn(label=True, text='11757578jfdjja')
  ]
))

# Search by the spam concept.
rows = dataset.select_rows(
  columns=['text', 'label'],
  searches=[
    ll.ConceptSearch(
      path='text',
      concept_namespace='lilac',
      concept_name='spam',
      embedding='gte-small')
  ],
  limit=1)

print(list(rows))
```

### üè∑Ô∏è Labeling

Lilac allows you to label individual points, or slices of data:
<img width="600" alt="image" src="docs/_static/dataset/dataset_add_label_tag.png">

We can also label all data given a filter. In this case, adding the label "short" to all text with a
small amount of characters. This field was produced by the automatic `text_statistics` signal.

<img width="600" alt="image" src="docs/_static/dataset/dataset_add_label_all_short.png">

We can do the same in Python:

```python
dataset.add_labels(
  'short',
  filters=[
    (('text', 'text_statistics', 'num_characters'), 'less', 1000)
  ]
)
```

Labels can be exported for downstream tasks. Detailed documentation
[here](https://docs.lilacml.com/datasets/dataset_labels.html).

## üí¨ Contact

For bugs and feature requests, please
[file an issue on GitHub](https://github.com/lilacai/lilac/issues).

For general questions, please [visit our Discord](https://discord.com/invite/jNzw9mC8pp).

