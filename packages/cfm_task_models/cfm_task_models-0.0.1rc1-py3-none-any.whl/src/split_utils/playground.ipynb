{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loads checkpoint by local backend from path: obj_det/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth\n"
     ]
    }
   ],
   "source": [
    "# !mim download mmdet --config mask-rcnn_swin-t-p4-w7_fpn_1x_coco --dest .\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmengine.config import Config\n",
    "\n",
    "from utils import SplitSwinTransformer, SplitTwoStageDetector, TwoInputIdentity\n",
    "\n",
    "config_file = 'obj_det/mask-rcnn_swin-t-p4-w7_fpn_1x_coco.py'\n",
    "checkpoint_file = 'obj_det/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth'\n",
    "\n",
    "model_init = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "# result = inference_detector(model, 'demo/demo.jpg')\n",
    "\n",
    "# from mmdet.registry import MODELS\n",
    "# MODELS.get('SwinTransformer')\n",
    "# MODELS.get('TwoStageDetector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('obj_det/mask-rcnn_swin-t-p4-w7_fpn_1x_coco.py')\n",
    "model = SplitTwoStageDetector.create_from_instance_and_cfg(model_init, cfg, cut_point=1)\n",
    "\n",
    "prp = model.data_preprocessor\n",
    "model.data_preprocessor = TwoInputIdentity()\n",
    "\n",
    "# model.backbone = SplitSwinTransformer.create_from_instance_and_cfg(model.backbone, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: obj_det/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth\n",
      "[tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True), tensor(True)]\n"
     ]
    }
   ],
   "source": [
    "model2 = SplitTwoStageDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/23 17:01:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.5 (default, May 30 2022, 18:17:32) [GCC 7.5.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1017988511\n",
      "    GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "    CUDA_HOME: /usr/local/cuda-10.1\n",
      "    NVCC: Cuda compilation tools, release 10.1, V10.1.24\n",
      "    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.9.0\n",
      "    MMEngine: 0.10.3\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    seed: 1017988511\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "02/23 17:01:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "max_epochs = 12\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        attn_drop_rate=0.0,\n",
      "        convert_weights=True,\n",
      "        depths=[\n",
      "            2,\n",
      "            2,\n",
      "            6,\n",
      "            2,\n",
      "        ],\n",
      "        drop_path_rate=0.2,\n",
      "        drop_rate=0.0,\n",
      "        embed_dims=96,\n",
      "        init_cfg=None,\n",
      "        mlp_ratio=4,\n",
      "        num_heads=[\n",
      "            3,\n",
      "            6,\n",
      "            12,\n",
      "            24,\n",
      "        ],\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        patch_norm=True,\n",
      "        qk_scale=None,\n",
      "        qkv_bias=True,\n",
      "        type='SwinTransformer',\n",
      "        window_size=7,\n",
      "        with_cp=False),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_mask=True,\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            96,\n",
      "            192,\n",
      "            384,\n",
      "            768,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=dict(\n",
      "            bbox_coder=dict(\n",
      "                target_means=[\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                ],\n",
      "                target_stds=[\n",
      "                    0.1,\n",
      "                    0.1,\n",
      "                    0.2,\n",
      "                    0.2,\n",
      "                ],\n",
      "                type='DeltaXYWHBBoxCoder'),\n",
      "            fc_out_channels=1024,\n",
      "            in_channels=256,\n",
      "            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "            loss_cls=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "            num_classes=80,\n",
      "            reg_class_agnostic=False,\n",
      "            roi_feat_size=7,\n",
      "            type='Shared2FCBBoxHead'),\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        mask_head=dict(\n",
      "            conv_out_channels=256,\n",
      "            in_channels=256,\n",
      "            loss_mask=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_mask=True),\n",
      "            num_classes=80,\n",
      "            num_convs=4,\n",
      "            type='FCNMaskHead'),\n",
      "        mask_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=14, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        test_cfg=dict(\n",
      "            mask_thr_binary=0.5,\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        train_cfg=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        type='StandardRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            mask_thr_binary=0.5,\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        rpn=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=1000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        rpn=dict(\n",
      "            allowed_border=-1,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='MaskRCNN')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ), lr=0.0001, type='AdamW', weight_decay=0.05),\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0))),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=1000, start_factor=0.001,\n",
      "        type='LinearLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=12,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            8,\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric=[\n",
      "        'bbox',\n",
      "        'segm',\n",
      "    ],\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_train2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='train2017/'),\n",
      "        data_root='data/coco/',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric=[\n",
      "        'bbox',\n",
      "        'segm',\n",
      "    ],\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = '.'\n",
      "\n",
      "02/23 17:01:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "02/23 17:01:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "from utils import get_train_pipeline_cfg\n",
    "\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "cfg = model_init.cfg.copy()\n",
    "test_pipeline = get_test_pipeline_cfg(cfg)\n",
    "\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "train_pipeline = get_train_pipeline_cfg(cfg)\n",
    "\n",
    "train_pipeline = Compose(train_pipeline)\n",
    "# data_ = dict(img_path='demo/demo.jpg', img_id=0)\n",
    "\n",
    "\n",
    "cfg_tr = cfg.copy()\n",
    "cfg_tr['work_dir'] = './logs'\n",
    "runner = RUNNERS.build(cfg_tr)\n",
    "# with open('data/coco/annotations/instances_train2017.json', 'r') as file:\n",
    "#     data_json = json.load(file)\n",
    "\n",
    "\n",
    "# data_ = train_pipeline(data_json)\n",
    "\n",
    "# data_['inputs'] = data_['inputs'].unsqueeze(0)\n",
    "# data_['data_samples'] = [data_['data_samples']]\n",
    "\n",
    "# print(train_pipeline)\n",
    "# print(list(data_json.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(tensor(0.3157, grad_fn=<AddBackward0>), OrderedDict([('loss', tensor(0.3157, grad_fn=<AddBackward0>)), ('loss_rpn_cls', tensor(0.0008, grad_fn=<AddBackward0>)), ('loss_rpn_bbox', tensor(0.0214, grad_fn=<AddBackward0>)), ('loss_cls', tensor(0.0573, grad_fn=<MeanBackward0>)), ('acc', tensor(99.0234)), ('loss_bbox', tensor(0.1031, grad_fn=<MeanBackward0>)), ('loss_mask', tensor(0.1332, grad_fn=<MeanBackward0>))]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dl = runner.train_dataloader\n",
    "print(model.cut_point)\n",
    "for i,data in enumerate(dl):\n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "    # im['inputs'] = torch.stack(im['inputs'])\n",
    "    # print(list(data_.keys()))\n",
    "    # data_ = train_pipeline(im)\n",
    "    # data_ = prp(data_, False)\n",
    "    # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    # data_['inputs'] = {\n",
    "    #                \"hw_shape\": data_['inputs'][0],\n",
    "    #                \"outs\": data_['inputs'][1]}\n",
    "    data = model.feature_frontend(data)\n",
    "    print(model.backend_loss(data))\n",
    "    # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "    # loss, losses = model.parse_losses(losses)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# data_ = prp(data_, False)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# data_['inputs'] = model.backbone.split_forward(data_['inputs'], output_layer=model.cut_point-1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# data_['inputs'] = {\"x\": data_['inputs'][0],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m# # forward the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtest_step(data_)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/model/base_model/base_model.py:145\u001b[0m, in \u001b[0;36mBaseModel.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m    list: The predictions of given data.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_preprocessor(data, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_forward(data, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/model/base_model/base_model.py:361\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, mode\u001b[39m=\u001b[39;49mmode)\n\u001b[1;32m    362\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    363\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39mdata, mode\u001b[39m=\u001b[39mmode)\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/detectors/base.py:94\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(inputs, data_samples)\n\u001b[1;32m     93\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(inputs, data_samples)\n\u001b[1;32m     95\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(inputs, data_samples)\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/detectors/two_stage.py:227\u001b[0m, in \u001b[0;36mTwoStageDetector.predict\u001b[0;34m(self, batch_inputs, batch_data_samples, rescale)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict results from a batch of inputs and data samples with post-\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mprocessing.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m        - masks (Tensor): Has a shape (num_instances, H, W).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_bbox, \u001b[39m'\u001b[39m\u001b[39mBbox head must be implemented.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 227\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_feat(batch_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[39m# If there are no pre-defined proposals, use RPN to get proposals\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m batch_data_samples[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mproposals\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/utils.py:99\u001b[0m, in \u001b[0;36mSplitTwoStageDetector.extract_feat\u001b[0;34m(self, batch_inputs)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone\u001b[39m.\u001b[39msplit_forward(x, hw_shape, outs\u001b[39m=\u001b[39mouts, input_layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcut_point)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_neck:\n\u001b[0;32m---> 99\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneck(x)\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/necks/fpn.py:171\u001b[0m, in \u001b[0;36mFPN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tuple[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward function.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m        tuple: Feature maps, each is a 4D-tensor.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_channels)\n\u001b[1;32m    173\u001b[0m     \u001b[39m# build laterals\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     laterals \u001b[39m=\u001b[39m [\n\u001b[1;32m    175\u001b[0m         lateral_conv(inputs[i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_level])\n\u001b[1;32m    176\u001b[0m         \u001b[39mfor\u001b[39;00m i, lateral_conv \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlateral_convs)\n\u001b[1;32m    177\u001b[0m     ]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data_ = prp(data_, False)\n",
    "# data_['inputs'] = model.backbone.split_forward(data_['inputs'], output_layer=model.cut_point-1)\n",
    "# data_['inputs'] = {\"x\": data_['inputs'][0],\n",
    "                #    \"hw_shape\": data_['inputs'][1],\n",
    "                #    \"outs\": data_['inputs'][2]}\n",
    "\n",
    "# data_['inputs'] = [data_['inputs']]\n",
    "# data_['data_samples'] = [data_['data_samples']]\n",
    "\n",
    "# # forward the model\n",
    "with torch.no_grad():\n",
    "    results = model.test_step(data_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinBlockSequence(\n",
       "  (blocks): ModuleList(\n",
       "    (0): SwinBlock(\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ShiftWindowMSA(\n",
       "        (w_msa): WindowMSA(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop): DropPath()\n",
       "      )\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dropout_layer): DropPath()\n",
       "        (gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): SwinBlock(\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ShiftWindowMSA(\n",
       "        (w_msa): WindowMSA(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop): DropPath()\n",
       "      )\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dropout_layer): DropPath()\n",
       "        (gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downsample): PatchMerging(\n",
       "    (adap_padding): AdaptivePadding()\n",
       "    (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.stages[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b048090fc6d3014cd2bdcc9ea88274151058c974d3dd655c4d5f3eddfe0b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
